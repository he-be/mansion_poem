The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998, 'pad_token_id': 200017}.
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1
   \\   /|    Num examples = 199 | Num Epochs = 20 | Total steps = 1,000
O^O/ \_/ \    Batch size per device = 1 | Gradient accumulation steps = 4
\        /    Data Parallel GPUs = 1 | Total batch size (1 x 4 x 1) = 4
 "-____-"     Trainable parameters = 3,981,312 of 20,918,738,496 (0.02% trained)
Unsloth: Will smartly offload gradients to save VRAM!
 [ 513/1000 1:19:15 < 1:15:31, 0.11 it/s, Epoch 10.24/20]
Step	Training Loss
4	4.098000
8	1.902400
12	1.456800
16	1.389100
20	1.300500
24	1.245000
28	1.230900
32	1.166300
36	1.228200
40	1.130700
44	1.065100
48	1.059500
52	1.042600
56	1.018000
60	1.012500
64	1.017400
68	1.065300
72	0.946200
76	0.920100
80	0.931100
84	1.038000
88	0.958800
92	0.934700
96	0.968500
100	0.918100
104	0.861600
108	0.876500
112	0.833500
116	0.844800
120	0.887600
124	0.902600
128	0.900000
132	0.910100
136	0.930000
140	0.871800
144	0.859300
148	0.823000
152	0.853000
156	0.794900
160	0.833900
164	0.775600
168	0.764300
172	0.795800
176	0.792400
180	0.719900
184	0.779700
188	0.805600
192	0.791900
196	0.838100
200	0.847200
204	0.756100
208	0.700700
212	0.740800
216	0.681100
220	0.659500
224	0.730700
228	0.681700
232	0.760800
236	0.750100
240	0.726200
244	0.724800
248	0.740100
252	0.666600
256	0.624200
260	0.613200
264	0.691900
268	0.666100
272	0.591900
276	0.647300
280	0.650000
284	0.726000
288	0.613100
292	0.634400
296	0.659100
300	0.652300
304	0.551600
308	0.556200
312	0.587400
316	0.588500
320	0.560800
324	0.591500
328	0.568700
332	0.565200
336	0.567400
340	0.563500
344	0.606000
348	0.590800
352	0.555400
356	0.518000
360	0.480900
364	0.493300
368	0.515000
372	0.480100
376	0.503400
380	0.526000
384	0.511700
388	0.516400
392	0.513700
396	0.528200
400	0.498700
404	0.441100
408	0.418500
412	0.443600
416	0.396500
420	0.405400
424	0.454000
428	0.433400
432	0.438300
436	0.452700
440	0.459100
444	0.429400
448	0.452700
452	0.408000
456	0.397600
460	0.354600
464	0.366000
468	0.391400
472	0.369400
476	0.349000
480	0.364700
484	0.368200
488	0.374300
492	0.368600
496	0.418400
500	0.378000
504	0.334900
508	0.315000
