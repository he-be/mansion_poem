#!/usr/bin/env python3
"""
ãƒ­ã‚°åˆ†æãƒ„ãƒ¼ãƒ« - LLMãƒ¢ãƒ‡ãƒ«ã®ç”Ÿæˆå“è³ªã‚’è©•ä¾¡

Google Gemini 2.5 Flash (OpenRouterçµŒç”±) ã‚’ä½¿ç”¨ã—ã¦ã€
å„LLMãƒ¢ãƒ‡ãƒ«ã®ç”Ÿæˆã—ãŸãƒã‚¨ãƒ ã‚’æ¡ç‚¹ã—ã€ãƒ¢ãƒ‡ãƒ«åˆ¥ã®æ¯”è¼ƒãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆã™ã‚‹ã€‚
"""

import sqlite3
import json
import os
import sys
from pathlib import Path
from typing import Dict, List, Any
from datetime import datetime
import requests
from dotenv import load_dotenv

# .envãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€
load_dotenv()

# è¨­å®š
PROJECT_ROOT = Path(__file__).parent.parent
DB_PATH = PROJECT_ROOT / "dev-server" / "dev-logs.db"
REPORT_PATH = PROJECT_ROOT / "docs" / "llm_analysis_report.md"

# OpenRouterè¨­å®šï¼ˆGemini 2.5 Flashå›ºå®šï¼‰
OPENROUTER_API_KEY = os.getenv("OPENROUTER_API_KEY")
EVALUATOR_MODEL = "google/gemini-2.5-flash-preview-09-2025"
OPENROUTER_URL = "https://openrouter.ai/api/v1/chat/completions"

# æ¡ç‚¹åŸºæº–ï¼ˆåˆè¨ˆ100ç‚¹ï¼‰
SCORING_CRITERIA = {
    "material_concealment": {
        "name": "ç´ æã®éš è”½",
        "weight": 40,
        "description": "å…ƒã®ãƒã‚¬ãƒ†ã‚£ãƒ–æ¡ä»¶ï¼ˆåŒ—å‘ãã€å´–ã€å€Ÿåœ°æ¨©ç­‰ï¼‰ã‚’æ¨æ¸¬ã•ã›ãªã„"
    },
    "unity_narrative": {
        "name": "çµ±ä¸€æ€§ãƒ»ç‰©èªæ€§",
        "weight": 20,
        "description": "ä¸€ã¤ã®ãƒ†ãƒ¼ãƒã§çµ±åˆã•ã‚Œã€ç‰©èªã¨ã—ã¦æˆç«‹ã—ã¦ã„ã‚‹"
    },
    "writing_style": {
        "name": "æ–‡ä½“ã®é©åˆ‡æ€§",
        "weight": 15,
        "description": "çŸ­æ–‡ã€ä½“è¨€æ­¢ã‚ã€ä¸»èªçœç•¥ã€ãƒªã‚ºãƒ ã®è‰¯ã•"
    },
    "prohibition_compliance": {
        "name": "ç¦æ­¢äº‹é …å›é¿",
        "weight": 15,
        "description": "ä¸å¯§èªãƒ»æ¨é‡ãƒ»ç›´æ¥å‘¼ã³ã‹ã‘ãƒ»æœ€ä¸Šç´šè¡¨ç¾ã‚’é¿ã‘ã‚‹"
    },
    "character_count": {
        "name": "æ–‡å­—æ•°é©æ­£",
        "weight": 10,
        "description": "180-240æ–‡å­—ã®ç¯„å›²å†…ï¼ˆÂ±10æ–‡å­—ã¯æ¸›ç‚¹ãªã—ï¼‰"
    }
}


def get_evaluation_prompt(selected_cards: List[Dict], generated_poem: Dict[str, str]) -> str:
    """æ¡ç‚¹ç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ç”Ÿæˆ"""

    # ã‚«ãƒ¼ãƒ‰æƒ…å ±ã‚’æ•´å½¢
    cards_text = ""
    for i, card in enumerate(selected_cards, 1):
        condition = card.get("conditionCard", {})
        poem_card = card.get("selectedPoem", {})
        cards_text += f"{i}. [{condition.get('category', 'N/A')}] {condition.get('condition_text', 'N/A')}\n"
        cards_text += f"   â†’ ãƒã‚¨ãƒ : {poem_card.get('poem_text', 'N/A')}\n"

    title = generated_poem.get("title", "")
    poem_text = generated_poem.get("poem", "")

    prompt = f"""ã‚ãªãŸã¯ä¸€æµä¸å‹•ç”£åºƒå‘Šã®ã‚¯ãƒªã‚¨ã‚¤ãƒ†ã‚£ãƒ–ãƒ‡ã‚£ãƒ¬ã‚¯ã‚¿ãƒ¼ã¨ã—ã¦ã€ä»¥ä¸‹ã®ç”Ÿæˆã•ã‚ŒãŸãƒãƒ³ã‚·ãƒ§ãƒ³ãƒã‚¨ãƒ ã‚’å³æ ¼ã«æ¡ç‚¹ã—ã¦ãã ã•ã„ã€‚

ã€å…ƒã®é¸æŠã•ã‚ŒãŸã‚«ãƒ¼ãƒ‰ãƒšã‚¢ã€‘
{cards_text}

ã€ç”Ÿæˆã•ã‚ŒãŸãƒã‚¨ãƒ ã€‘
ã‚¿ã‚¤ãƒˆãƒ«: {title}
æœ¬æ–‡:
{poem_text}

ã€æ¡ç‚¹åŸºæº–ã€‘ï¼ˆåˆè¨ˆ100ç‚¹æº€ç‚¹ï¼‰

1. **ç´ æã®éš è”½** (40ç‚¹)
   - å…ƒã®ãƒã‚¬ãƒ†ã‚£ãƒ–æ¡ä»¶ï¼ˆã€ŒåŒ—å‘ãã€ã€Œå´–ã€ã€Œå€Ÿåœ°æ¨©ã€ã€Œãƒã‚¹ã§é§…ã¸10åˆ†ã€ç­‰ï¼‰ãŒç”Ÿæˆæ–‡ã‹ã‚‰æ¨æ¸¬ã§ãã‚‹ã‹ï¼Ÿ
   - å®Œå…¨ã«éš è”½ã§ãã¦ã„ã‚‹: 40ç‚¹
   - ã‚„ã‚„åŒ‚ã‚ã›ã¦ã„ã‚‹: 20-30ç‚¹
   - æ˜ç¤ºçš„ã«è¨€åŠ: 0-10ç‚¹

2. **çµ±ä¸€æ€§ãƒ»ç‰©èªæ€§** (20ç‚¹)
   - ã‚«ãƒ¼ãƒ‰ã®ã€Œé­‚ã€ã‚’æŠ½å‡ºã—ã€ä¸€ã¤ã®ç‰©èªã¨ã—ã¦å†å‰µé€ ã§ãã¦ã„ã‚‹ã‹ï¼Ÿ
   - å˜ãªã‚‹ä¸¦åˆ—ã§ã¯ãªãã€ãƒ†ãƒ¼ãƒã§çµ±åˆã•ã‚Œã¦ã„ã‚‹ã‹ï¼Ÿ

3. **æ–‡ä½“ã®é©åˆ‡æ€§** (15ç‚¹)
   - çŸ­æ–‡ãƒ»ä½“è¨€æ­¢ã‚ãƒ»ä¸»èªçœç•¥ãŒé©åˆ‡ã«ä½¿ã‚ã‚Œã¦ã„ã‚‹ã‹ï¼Ÿ
   - ãƒªã‚ºãƒ ã¨ä½™éŸ»ãŒã‚ã‚‹ã‹ï¼Ÿ

4. **ç¦æ­¢äº‹é …å›é¿** (15ç‚¹)
   - ã€Œã§ã™ã€ã€Œã§ã—ã‚‡ã†ã€ç­‰ã®ä¸å¯§èªãƒ»æ¨é‡ã‚’ä½¿ã£ã¦ã„ãªã„ã‹ï¼Ÿ
   - ã€Œã‚ãªãŸã€ç­‰ã®ç›´æ¥å‘¼ã³ã‹ã‘ã‚’é¿ã‘ã¦ã„ã‚‹ã‹ï¼Ÿ
   - ã€Œæœ€é«˜ã€ã€Œå®Œç’§ã€ç­‰ã®æœ€ä¸Šç´šè¡¨ç¾ã‚’é¿ã‘ã¦ã„ã‚‹ã‹ï¼Ÿ

5. **æ–‡å­—æ•°é©æ­£** (10ç‚¹)
   - 180-240æ–‡å­—ï¼ˆÂ±10æ–‡å­—ã¯æ¸›ç‚¹ãªã—ï¼‰
   - ç¾åœ¨ã®æ–‡å­—æ•°: {len(poem_text)}æ–‡å­—

ã€å‡ºåŠ›å½¢å¼ã€‘
ä»¥ä¸‹ã®JSONå½¢å¼ã§å‡ºåŠ›ã—ã¦ãã ã•ã„ï¼ˆèª¬æ˜ä¸è¦ï¼‰:
```json
{{
  "total_score": 85,
  "scores": {{
    "material_concealment": 38,
    "unity_narrative": 18,
    "writing_style": 13,
    "prohibition_compliance": 14,
    "character_count": 10
  }},
  "comment": "ç·åˆè©•ä¾¡ã®ã‚³ãƒ¡ãƒ³ãƒˆï¼ˆ100æ–‡å­—ä»¥å†…ï¼‰",
  "strengths": "å„ªã‚Œã¦ã„ã‚‹ç‚¹ï¼ˆ50æ–‡å­—ä»¥å†…ï¼‰",
  "improvements": "æ”¹å–„ã™ã¹ãç‚¹ï¼ˆ50æ–‡å­—ä»¥å†…ï¼‰"
}}
```"""

    return prompt


def evaluate_with_gemini(prompt: str) -> Dict[str, Any]:
    """Gemini 2.5 Flashã§LLMç”Ÿæˆç‰©ã‚’è©•ä¾¡"""

    if not OPENROUTER_API_KEY:
        raise ValueError("OPENROUTER_API_KEY is not set in .env file")

    headers = {
        "Authorization": f"Bearer {OPENROUTER_API_KEY}",
        "Content-Type": "application/json",
        "HTTP-Referer": "http://localhost:3001",
        "X-Title": "Mansion Poem Log Analyzer"
    }

    payload = {
        "model": EVALUATOR_MODEL,
        "messages": [
            {
                "role": "user",
                "content": prompt
            }
        ],
        "temperature": 0.3,  # æ¡ç‚¹ã¯ä¸€è²«æ€§é‡è¦–
        "max_tokens": 2048
    }

    try:
        response = requests.post(OPENROUTER_URL, headers=headers, json=payload, timeout=30)
        response.raise_for_status()

        data = response.json()
        content = data["choices"][0]["message"]["content"]

        # JSONã‚’æŠ½å‡º
        json_match = content.strip()
        if "```json" in json_match:
            json_match = json_match.split("```json")[1].split("```")[0].strip()

        return json.loads(json_match)

    except Exception as e:
        print(f"Error evaluating with Gemini: {e}")
        return {
            "total_score": 0,
            "scores": {k: 0 for k in SCORING_CRITERIA.keys()},
            "comment": f"è©•ä¾¡ã‚¨ãƒ©ãƒ¼: {str(e)}",
            "strengths": "N/A",
            "improvements": "N/A"
        }


def load_logs_from_db() -> List[Dict[str, Any]]:
    """SQLiteãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰ãƒ­ã‚°ã‚’èª­ã¿è¾¼ã‚€"""

    if not DB_PATH.exists():
        print(f"Error: Database not found at {DB_PATH}")
        sys.exit(1)

    conn = sqlite3.connect(DB_PATH)
    cursor = conn.cursor()

    cursor.execute("""
        SELECT
            id, created_at, selected_cards, generated_poem,
            generation_time_ms, is_successful,
            llm_provider, llm_model, prompt_text
        FROM generation_logs
        WHERE is_successful = 1
        ORDER BY created_at DESC
    """)

    logs = []
    for row in cursor.fetchall():
        logs.append({
            "id": row[0],
            "created_at": row[1],
            "selected_cards": json.loads(row[2]),
            "generated_poem": json.loads(row[3]),
            "generation_time_ms": row[4],
            "is_successful": bool(row[5]),
            "llm_provider": row[6],
            "llm_model": row[7],
            "prompt_text": row[8]
        })

    conn.close()
    return logs


def analyze_logs():
    """ãƒ­ã‚°ã‚’åˆ†æã—ã¦ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ"""

    print("ğŸ“Š ãƒ­ã‚°åˆ†æã‚’é–‹å§‹ã—ã¾ã™...")
    print(f"ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹: {DB_PATH}")
    print(f"è©•ä¾¡ãƒ¢ãƒ‡ãƒ«: {EVALUATOR_MODEL}\n")

    # ãƒ­ã‚°ã‚’èª­ã¿è¾¼ã‚€
    logs = load_logs_from_db()
    print(f"âœ“ {len(logs)}ä»¶ã®æˆåŠŸãƒ­ã‚°ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ\n")

    if len(logs) == 0:
        print("è©•ä¾¡å¯¾è±¡ã®ãƒ­ã‚°ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
        sys.exit(0)

    # ãƒ¢ãƒ‡ãƒ«åˆ¥ã«è©•ä¾¡
    results_by_model = {}

    for i, log in enumerate(logs, 1):
        model_name = log["llm_model"]

        print(f"[{i}/{len(logs)}] è©•ä¾¡ä¸­: {model_name} (ID: {log['id']})")

        # è©•ä¾¡ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ç”Ÿæˆ
        eval_prompt = get_evaluation_prompt(
            log["selected_cards"],
            log["generated_poem"]
        )

        # Geminiã§è©•ä¾¡
        evaluation = evaluate_with_gemini(eval_prompt)

        # çµæœã‚’ä¿å­˜
        if model_name not in results_by_model:
            results_by_model[model_name] = []

        results_by_model[model_name].append({
            "log_id": log["id"],
            "created_at": log["created_at"],
            "poem": log["generated_poem"],
            "evaluation": evaluation,
            "generation_time_ms": log["generation_time_ms"]
        })

        print(f"   â†’ ã‚¹ã‚³ã‚¢: {evaluation['total_score']}/100\n")

    # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
    generate_report(results_by_model)

    print(f"\nâœ“ åˆ†æå®Œäº†ï¼")
    print(f"ãƒ¬ãƒãƒ¼ãƒˆ: {REPORT_PATH}")


def generate_report(results_by_model: Dict[str, List[Dict]]):
    """Markdownãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ"""

    report_lines = [
        "# LLMãƒ¢ãƒ‡ãƒ«ç”Ÿæˆå“è³ªåˆ†æãƒ¬ãƒãƒ¼ãƒˆ",
        "",
        f"ç”Ÿæˆæ—¥æ™‚: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
        f"è©•ä¾¡ãƒ¢ãƒ‡ãƒ«: `{EVALUATOR_MODEL}`",
        "",
        "---",
        ""
    ]

    # ã‚µãƒãƒªãƒ¼
    report_lines.append("## ğŸ“Š ç·åˆã‚µãƒãƒªãƒ¼")
    report_lines.append("")
    report_lines.append("| ãƒ¢ãƒ‡ãƒ« | è©•ä¾¡ä»¶æ•° | å¹³å‡ã‚¹ã‚³ã‚¢ | æ¨™æº–åå·® | æœ€é«˜ç‚¹ | æœ€ä½ç‚¹ | å¹³å‡ç”Ÿæˆæ™‚é–“ |")
    report_lines.append("|--------|----------|------------|----------|--------|--------|--------------|")

    for model_name, results in sorted(results_by_model.items()):
        scores = [r["evaluation"]["total_score"] for r in results]
        times = [r["generation_time_ms"] for r in results]

        avg_score = sum(scores) / len(scores)
        std_dev = (sum((s - avg_score) ** 2 for s in scores) / len(scores)) ** 0.5

        report_lines.append(
            f"| `{model_name}` | {len(results)} | "
            f"{avg_score:.1f} | {std_dev:.1f} | "
            f"{max(scores)} | {min(scores)} | {sum(times) / len(times):.0f}ms |"
        )

    report_lines.append("")
    report_lines.append("---")
    report_lines.append("")

    # ãƒ¢ãƒ‡ãƒ«åˆ¥è©³ç´°
    for model_name, results in sorted(results_by_model.items()):
        report_lines.append(f"## ğŸ” {model_name}")
        report_lines.append("")

        # æ¡ç‚¹åŸºæº–åˆ¥ã®å¹³å‡
        criteria_scores = {k: [] for k in SCORING_CRITERIA.keys()}
        for result in results:
            for criterion, score in result["evaluation"]["scores"].items():
                criteria_scores[criterion].append(score)

        report_lines.append("### æ¡ç‚¹åŸºæº–åˆ¥å¹³å‡")
        report_lines.append("")
        report_lines.append("| æ¡ç‚¹åŸºæº– | é…ç‚¹ | å¹³å‡ç‚¹ | é”æˆç‡ |")
        report_lines.append("|----------|------|--------|--------|")

        for criterion_key, criterion_info in SCORING_CRITERIA.items():
            avg = sum(criteria_scores[criterion_key]) / len(criteria_scores[criterion_key])
            achievement = (avg / criterion_info["weight"]) * 100

            report_lines.append(
                f"| {criterion_info['name']} | {criterion_info['weight']}ç‚¹ | "
                f"{avg:.1f}ç‚¹ | {achievement:.0f}% |"
            )

        report_lines.append("")

        # å„ªç§€ä¾‹ãƒ»æ”¹å–„ä¾‹
        sorted_results = sorted(results, key=lambda r: r["evaluation"]["total_score"], reverse=True)

        if len(sorted_results) > 0:
            best = sorted_results[0]
            report_lines.append(f"### âœ¨ å„ªç§€ä¾‹ï¼ˆã‚¹ã‚³ã‚¢: {best['evaluation']['total_score']}/100ï¼‰")
            report_lines.append("")
            report_lines.append(f"**ã‚¿ã‚¤ãƒˆãƒ«**: {best['poem']['title']}")
            report_lines.append("")
            report_lines.append(f"**æœ¬æ–‡**:")
            report_lines.append(f"```")
            report_lines.append(best['poem']['poem'])
            report_lines.append(f"```")
            report_lines.append("")
            report_lines.append(f"**è©•ä¾¡**: {best['evaluation']['comment']}")
            report_lines.append(f"**å¼·ã¿**: {best['evaluation']['strengths']}")
            report_lines.append("")

        if len(sorted_results) > 1:
            worst = sorted_results[-1]
            report_lines.append(f"### ğŸ“ æ”¹å–„ä¾‹ï¼ˆã‚¹ã‚³ã‚¢: {worst['evaluation']['total_score']}/100ï¼‰")
            report_lines.append("")
            report_lines.append(f"**ã‚¿ã‚¤ãƒˆãƒ«**: {worst['poem']['title']}")
            report_lines.append("")
            report_lines.append(f"**æœ¬æ–‡**:")
            report_lines.append(f"```")
            report_lines.append(worst['poem']['poem'])
            report_lines.append(f"```")
            report_lines.append("")
            report_lines.append(f"**è©•ä¾¡**: {worst['evaluation']['comment']}")
            report_lines.append(f"**æ”¹å–„ç‚¹**: {worst['evaluation']['improvements']}")
            report_lines.append("")

        report_lines.append("---")
        report_lines.append("")

    # ãƒ•ã‚¡ã‚¤ãƒ«ã«æ›¸ãè¾¼ã¿
    REPORT_PATH.parent.mkdir(parents=True, exist_ok=True)
    REPORT_PATH.write_text("\n".join(report_lines), encoding="utf-8")


if __name__ == "__main__":
    analyze_logs()
